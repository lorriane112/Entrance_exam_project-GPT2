import json
import torch
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from model.nanogpt_model import GPT
from config.model_config import GPTConfig
from tokenizers import Tokenizer

class ArticleContinuer:
    def __init__(self, model_path='model_best.pth'):
        print("ğŸ“– åˆå§‹åŒ–æ–‡ç« ç»­å†™å™¨...")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = Tokenizer.from_file("data/tokenizer.json")
        vocab_size = self.tokenizer.get_vocab_size()
        
        # åŠ è½½æ¨¡å‹
        config = GPTConfig(
            vocab_size=vocab_size,
            block_size=256,
            n_layer=12,
            n_head=12,
            n_embd=768
        )
        
        self.model = GPT(config)
        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def extract_full_article(self, json_data):
        """ä»JSONä¸­æå–å®Œæ•´çš„æ–‡ç« å†…å®¹"""
        full_text = ""
        
        def extract_text(obj):
            nonlocal full_text
            if isinstance(obj, str):
                full_text += obj + "\n"
            elif isinstance(obj, dict):
                for value in obj.values():
                    extract_text(value)
            elif isinstance(obj, list):
                for item in obj:
                    extract_text(item)
        
        extract_text(json_data)
        return full_text.strip()
    
    def continue_article_end(self, article_text, num_sentences=10, context_length=500):
        """åœ¨æ–‡ç« æœ«å°¾ç»­å†™æŒ‡å®šæ•°é‡çš„å¥å­"""
        print(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(article_text)} å­—ç¬¦")
        
        # å–æ–‡ç« æœ€åéƒ¨åˆ†ä½œä¸ºä¸Šä¸‹æ–‡
        if len(article_text) > context_length:
            context = article_text[-context_length:]
            print(f"ä½¿ç”¨æœ€å {context_length} å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡")
        else:
            context = article_text
            print(f"ä½¿ç”¨å…¨æ–‡ä½œä¸ºä¸Šä¸‹æ–‡")
        
        print(f"ä¸Šä¸‹æ–‡å†…å®¹: ...{context[-100:]}")
        
        # å‡†å¤‡è¾“å…¥
        input_ids = torch.tensor([self.tokenizer.encode(context).ids])
        generated_ids = input_ids.clone()
        
        print(f"\nğŸ¯ å¼€å§‹ç»­å†™ {num_sentences} ä¸ªå¥å­...")
        
        sentences_generated = 0
        continuation_text = ""
        
        with torch.no_grad():
            while sentences_generated < num_sentences:
                # ç¡®ä¿è¾“å…¥ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶
                if generated_ids.size(1) >= self.model.config.block_size:
                    generated_ids = generated_ids[:, -self.model.config.block_size:]
                
                # å‰å‘ä¼ æ’­
                logits, _ = self.model(generated_ids)
                next_token_logits = logits[:, -1, :] / 0.8  # temperature=0.8
                
                # Top-k é‡‡æ ·
                top_k = 50
                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                next_token_logits[indices_to_remove] = -float('inf')
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
                generated_ids = torch.cat([generated_ids, next_token], dim=1)
                
                # è§£ç å½“å‰ç”Ÿæˆçš„å†…å®¹
                current_full = self.tokenizer.decode(generated_ids[0].tolist())
                current_continuation = current_full[len(context):]
                
                # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–°çš„å¥å­ï¼ˆä¸­æ–‡å¥å­ç»“æŸç¬¦ï¼‰
                sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦â€¦']
                if any(marker in current_continuation for marker in sentence_endings):
                    # ç»Ÿè®¡å¥å­æ•°é‡
                    new_sentences = 0
                    for marker in sentence_endings:
                        new_sentences += current_continuation.count(marker)
                    
                    if new_sentences > sentences_generated:
                        sentences_generated = new_sentences
                        continuation_text = current_continuation
                        print(f"âœ… å·²ç”Ÿæˆ {sentences_generated}/{num_sentences} ä¸ªå¥å­")
                
                # å®‰å…¨åœæ­¢ï¼šå¦‚æœç”Ÿæˆå¤ªé•¿ä½†å¥å­ä¸å¤Ÿ
                if generated_ids.size(1) - input_ids.size(1) > 500:  # æœ€å¤šç”Ÿæˆ500ä¸ªtoken
                    print("âš ï¸  è¾¾åˆ°ç”Ÿæˆé•¿åº¦é™åˆ¶ï¼Œæå‰åœæ­¢")
                    break
        
        return continuation_text
    
    def save_continued_article(self, original_text, continuation, output_file):
        """ä¿å­˜ç»­å†™åçš„å®Œæ•´æ–‡ç« """
        full_article = original_text + "\n\n" + "="*50 + "\nã€ç»­å†™éƒ¨åˆ†ã€‘\n" + "="*50 + "\n" + continuation
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(full_article)
        
        # åŒæ—¶ä¿å­˜JSONæ ¼å¼çš„ç»“æœ
        result_data = {
            "original_article": original_text,
            "continuation": continuation,
            "full_article": original_text + continuation
        }
        
        json_output_file = output_file.replace('.txt', '.json')
        with open(json_output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        return full_article

def main():
    # åˆå§‹åŒ–ç»­å†™å™¨
    continuer = ArticleContinuer()
    
    # æ‚¨çš„JSONæ–‡ä»¶è·¯å¾„
    json_file = "data/train.json" 
    
    if not os.path.exists(json_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        print("è¯·å°†æ‚¨çš„JSONæ–‡ä»¶æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œå¹¶ä¿®æ”¹è„šæœ¬ä¸­çš„æ–‡ä»¶è·¯å¾„")
        return
    
    # è¯»å–JSONæ–‡ä»¶
    print(f"ğŸ“š è¯»å–æ–‡ä»¶: {json_file}")
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–å®Œæ•´æ–‡ç« 
    article_text = continuer.extract_full_article(data)
    
    if not article_text:
        print("âŒ æœªèƒ½ä»JSONä¸­æå–åˆ°æ–‡æœ¬å†…å®¹")
        return
    
    print(f"ğŸ“– æå–åˆ°æ–‡ç« ï¼Œé•¿åº¦: {len(article_text)} å­—ç¬¦")
    print(f"æ–‡ç« ç»“å°¾: ...{article_text[-100:]}")
    
    # åœ¨æ–‡ç« æœ«å°¾ç»­å†™10ä¸ªå¥å­
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹ç»­å†™æ–‡ç« ç»“å°¾...")
    print("="*60)
    
    continuation = continuer.continue_article_end(
        article_text, 
        num_sentences=10,
        context_length=400  # ä½¿ç”¨æœ€å400å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
    )
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*60)
    print("ğŸ‰ ç»­å†™å®Œæˆï¼")
    print("="*60)
    
    print(f"\nğŸ“– åŸæ–‡ç»“å°¾:")
    print(f"...{article_text[-200:]}")
    
    print(f"\nâœ¨ ç»­å†™çš„10ä¸ªå¥å­:")
    print(continuation)
    
    # ä¿å­˜ç»“æœ
    output_file = "continued_article.txt"
    full_article = continuer.save_continued_article(article_text, continuation, output_file)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"  - {output_file} (æ–‡æœ¬æ ¼å¼)")
    print(f"  - {output_file.replace('.txt', '.json')} (JSONæ ¼å¼)")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  åŸæ–‡é•¿åº¦: {len(article_text)} å­—ç¬¦")
    print(f"  ç»­å†™é•¿åº¦: {len(continuation)} å­—ç¬¦")
    print(f"  ç»­å†™å¥å­æ•°: {continuation.count('ã€‚') + continuation.count('ï¼') + continuation.count('ï¼Ÿ')} ä¸ª")

if __name__ == "__main__":
    main()